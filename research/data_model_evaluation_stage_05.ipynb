{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_KEY=\"best_model\"\n",
    "MODEL_PATH_KEY=\"model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "ModelEvaluationArtifact=namedtuple(\"ModelEvaluationArtifact\",['evaluated_model_path', 'is_model_accepted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from insurence_premium.entity.model_factory import evaluate_regression_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\somit\\\\Downloads\\\\project_ineuron\\\\insurence_premium_prediction'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class DataModelEvaluation:\n",
    "    root_dir: Path\n",
    "    model_evaluation_file_path:Path\n",
    "    time_stamp: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from insurence_premium.constant import *\n",
    "from insurence_premium.util.common import (\n",
    "    \n",
    "    create_directories,read_yaml,\n",
    "    load_numpy_array_data,load_data,load_object,write_yaml_file\n",
    ")\n",
    "from pathlib import Path\n",
    "from insurence_premium.entity import (\n",
    "    DataIngestionConfig,\n",
    "    DataValidationConfig,\n",
    "    DataTransformationConfig,\n",
    "    DataModelTrainerConfig\n",
    ")\n",
    "from insurence_premium import logger\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH\n",
    "    ):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.time_stamp=CURRENT_TIME_STAMP\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_ingestion_config(\n",
    "        self,\n",
    "    ) -> DataIngestionConfig:\n",
    "        ingestion_config = self.config.data_ingestion\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=Path(ingestion_config.root_dir),\n",
    "            raw_data=Path(ingestion_config.raw_data),\n",
    "            ingested_train_dir=Path(ingestion_config.ingested_train_dir),\n",
    "            ingested_test_dir=Path(ingestion_config.ingested_test_dir),\n",
    "        )\n",
    "        return data_ingestion_config\n",
    "\n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        validation_config = self.config.data_validation\n",
    "\n",
    "        data_validation_config = DataValidationConfig(\n",
    "            root_dir=Path(validation_config.root_dir),\n",
    "            schema_file_path=Path(validation_config.schema_file_path),\n",
    "            report_file_path=Path(validation_config.report_file_path),\n",
    "            report_page_file_path=Path(validation_config.report_page_file_path),\n",
    "        )\n",
    "        return data_validation_config\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        try:\n",
    "            transformation_config = self.config.data_transformation\n",
    "            data_transformation_config = DataTransformationConfig(\n",
    "                root_dir=Path(transformation_config.root_dir),\n",
    "                transformed_train_file_path=Path(\n",
    "                    transformation_config.transformed_train_file_path\n",
    "                ),\n",
    "                transformed_test_file_path=Path(\n",
    "                    transformation_config.transformed_test_file_path\n",
    "                ),\n",
    "                preprocessing_dir=Path(transformation_config.preprocessing_dir),\n",
    "                preprocessing_file_path=Path(\n",
    "                    transformation_config.preprocessing_file_path\n",
    "                ),\n",
    "            )\n",
    "            return data_transformation_config\n",
    "            logging.info(f\"return: [{data_ingestion_config}]\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def get_model_trainer_config(self) -> DataModelTrainerConfig:\n",
    "        try:\n",
    "            model_trainer_config = self.config.data_model_trainer\n",
    "\n",
    "            data_model_trainer_config = DataModelTrainerConfig(\n",
    "                root_dir=Path(model_trainer_config.root_dir),\n",
    "                trained_model_file_path=\n",
    "                    model_trainer_config.trained_model_file_path\n",
    "                ,\n",
    "                base_accuracy=model_trainer_config.base_accuracy,\n",
    "                model_config_file_path=Path(\n",
    "                    model_trainer_config.model_config_file_path\n",
    "                ),\n",
    "            )\n",
    "            return data_model_trainer_config\n",
    "            logger.info(f\"model trainer config : {data_model_trainer_config}\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def get_model_evaluation_config(self):\n",
    "        try:\n",
    "            data_evaluation_config=self.config.data_model_evaluation\n",
    "            model_evaluation_file_path=os.path.join(data_evaluation_config.root_dir ,data_evaluation_config.model_evaluation_file_name)\n",
    "            model_evaluation_config=DataModelEvaluation(\n",
    "                root_dir= Path(data_evaluation_config.root_dir),\n",
    "                model_evaluation_file_path= Path(model_evaluation_file_path),\n",
    "                time_stamp= self.time_stamp\n",
    "\n",
    "                \n",
    "            )\n",
    "            return model_evaluation_config\n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-04-04 14:05:09,715: INFO: common]: yaml file: config\\config.yaml loaded successfully\n",
      "[2023-04-04 14:05:09,719: INFO: common]: yaml file: params.yaml loaded successfully\n",
      "[2023-04-04 14:05:09,724: INFO: common]: created directory at: artifacts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataModelTrainerConfig(root_dir=WindowsPath('artifacts/data_model_trainer'), trained_model_file_path='artifacts/data_model_trainer/trained_model/model.pkl', base_accuracy=0.6, model_config_file_path=WindowsPath('config/model.yaml'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConfigurationManager().get_model_trainer_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-04-04-14-02-29'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from insurence_premium.constant import *\n",
    "\n",
    "CURRENT_TIME_STAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-04-04 14:03:08,609: INFO: common]: yaml file: config\\config.yaml loaded successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConfigBox({'root_dir': 'artifacts/data_model_trainer', 'trained_model_file_path': 'artifacts/data_model_trainer/trained_model/model.pkl', 'base_accuracy': 0.6, 'model_config_file_path': 'config/model.yaml'})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_yaml(CONFIG_FILE_PATH).data_model_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-04-04 14:05:32,863: INFO: common]: yaml file: config\\config.yaml loaded successfully\n",
      "[2023-04-04 14:05:32,866: INFO: common]: yaml file: params.yaml loaded successfully\n",
      "[2023-04-04 14:05:32,869: INFO: common]: created directory at: artifacts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataModelTrainerConfig(root_dir=WindowsPath('artifacts/data_model_trainer'), trained_model_file_path='artifacts/data_model_trainer/trained_model/model.pkl', base_accuracy=0.6, model_config_file_path=WindowsPath('config/model.yaml'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConfigurationManager().get_model_trainer_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-04-04 14:05:36,103: INFO: common]: yaml file: config\\config.yaml loaded successfully\n",
      "[2023-04-04 14:05:36,110: INFO: common]: yaml file: params.yaml loaded successfully\n",
      "[2023-04-04 14:05:36,113: INFO: common]: created directory at: artifacts\n"
     ]
    }
   ],
   "source": [
    "class ModelEvalution:\n",
    "    def __init__(self,config:ConfigurationManager() ,model_trainer_artifact:ModelTrainerArtifact):\n",
    "        self.config=config\n",
    "        self.model_evaluation_config=self.config.get_model_evaluation_config()\n",
    "        self.model_trainer_config= self.config.get_model_trainer_config()\n",
    "        self.model_validation=self.config.get_data_validation_config()\n",
    "        self.model_ingestion=self.config.get_data_ingestion_config()\n",
    "        self.model_trainer_artifact=model_trainer_artifact\n",
    "    def get_best_model(self):\n",
    "        try:\n",
    "            \n",
    "            model = None\n",
    "            model_evaluation_file_path = self.model_evaluation_config.model_evaluation_file_path\n",
    "\n",
    "            if not os.path.exists(model_evaluation_file_path):\n",
    "                write_yaml_file(file_path=model_evaluation_file_path,\n",
    "                                )\n",
    "                return model\n",
    "            model_eval_file_content=read_yaml_file(file_path=model_evaluation_file_path)\n",
    "            logger.info(f\"model_eval_file read successfully\")\n",
    "\n",
    "            eval_content=dict() if model_eval_file_content is None else model_eval_file_content\n",
    "\n",
    "            if BEST_MODEL_KEY  not in eval_content:\n",
    "                return model\n",
    "            model=load_object(file_path=model_eval_file_content[BEST_MODEL_KEY][MODEL_PATH_KEY]) \n",
    "            return model\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def update_evaluation_report(self, model_evaluation_artifact: ModelEvaluationArtifact):\n",
    "        try:\n",
    "            eval_file_path = self.model_evaluation_config.model_evaluation_file_path\n",
    "            model_eval_content = read_yaml_file(file_path=eval_file_path)\n",
    "            model_eval_content = dict() if model_eval_content is None else model_eval_content\n",
    "            \n",
    "            \n",
    "            previous_best_model = None\n",
    "            if BEST_MODEL_KEY in model_eval_content:\n",
    "                previous_best_model = model_eval_content[BEST_MODEL_KEY]\n",
    "\n",
    "            logger.info(f\"Previous eval result: {model_eval_content}\")\n",
    "            eval_result = {\n",
    "                BEST_MODEL_KEY: {\n",
    "                    MODEL_PATH_KEY: model_evaluation_artifact.evaluated_model_path,\n",
    "                }\n",
    "            }\n",
    "\n",
    "            if previous_best_model is not None:\n",
    "                model_history = {self.model_evaluation_config.time_stamp: previous_best_model}\n",
    "                if HISTORY_KEY not in model_eval_content:\n",
    "                    history = {HISTORY_KEY: model_history}\n",
    "                    eval_result.update(history)\n",
    "                else:\n",
    "                    model_eval_content[HISTORY_KEY].update(model_history)\n",
    "\n",
    "            model_eval_content.update(eval_result)\n",
    "            logger.info(f\"Updated eval result:{model_eval_content}\")\n",
    "            write_yaml_file(file_path=eval_file_path, data=model_eval_content)\n",
    "        except Exception as e:\n",
    "            raise e    \n",
    "\n",
    "\n",
    "    def initiate_model_evaluation(self) -> ModelEvaluationArtifact:\n",
    "        try:\n",
    "            trained_model_file_path = self.model_trainer_config.trained_model_file_path\n",
    "            trained_model_object = load_object(file_path=trained_model_file_path)\n",
    "\n",
    "            train_file_path = self.model_ingestion.ingested_train_dir\n",
    "            test_file_path = self.model_ingestion.ingested_test_dir\n",
    "\n",
    "            schema_file_path = self.model_validation.schema_file_path\n",
    "\n",
    "            train_dataframe = load_data(file_path=train_file_path,\n",
    "                                                           schema_file_path=schema_file_path,\n",
    "                                                           )\n",
    "            test_dataframe = load_data(file_path=test_file_path,\n",
    "                                                          schema_file_path=schema_file_path,\n",
    "                                                          )\n",
    "            schema_content = read_yaml(path_to_yaml=schema_file_path)\n",
    "            target_column_name = schema_content[TARGET_COLUMN_KEY]\n",
    "\n",
    "            # target_column\n",
    "            logger.info(f\"Converting target column into numpy array.\")\n",
    "            train_target_arr = np.array(train_dataframe[target_column_name])\n",
    "            test_target_arr = np.array(test_dataframe[target_column_name])\n",
    "            logger.info(f\"Conversion completed target column into numpy array.\")\n",
    "\n",
    "            # dropping target column from the dataframe\n",
    "            logger.info(f\"Dropping target column from the dataframe.\")\n",
    "            train_dataframe.drop(target_column_name, axis=1, inplace=True)\n",
    "            test_dataframe.drop(target_column_name, axis=1, inplace=True)\n",
    "            logger.info(f\"Dropping target column from the dataframe completed.\")\n",
    "\n",
    "            model = self.get_best_model()\n",
    "\n",
    "            if model is None:\n",
    "                logger.info(\"Not found any existing model. Hence accepting trained model\")\n",
    "                model_evaluation_artifact = ModelEvaluationArtifact(evaluated_model_path=trained_model_file_path,\n",
    "                                                                    is_model_accepted=True)\n",
    "                self.update_evaluation_report(model_evaluation_artifact)\n",
    "                logger.info(f\"Model accepted. Model eval artifact {model_evaluation_artifact} created\")\n",
    "                return model_evaluation_artifact\n",
    "\n",
    "            model_list = [model, trained_model_object]\n",
    "\n",
    "            metric_info_artifact = evaluate_regression_model(model_list=model_list,\n",
    "                                                               X_train=train_dataframe,\n",
    "                                                               y_train=train_target_arr,\n",
    "                                                               X_test=test_dataframe,\n",
    "                                                               y_test=test_target_arr,\n",
    "                                                               base_accuracy=self.model_trainer_artifact.model_accuracy,\n",
    "                                                               )\n",
    "            logger.info(f\"Model evaluation completed. model metric artifact: {metric_info_artifact}\")\n",
    "\n",
    "            if metric_info_artifact is None:\n",
    "                response = ModelEvaluationArtifact(is_model_accepted=False,\n",
    "                                                   evaluated_model_path=trained_model_file_path\n",
    "                                                   )\n",
    "                logger.info(response)\n",
    "                return response\n",
    "\n",
    "            if metric_info_artifact.index_number == 1:\n",
    "                model_evaluation_artifact = ModelEvaluationArtifact(evaluated_model_path=trained_model_file_path,\n",
    "                                                                    is_model_accepted=True)\n",
    "                self.update_evaluation_report(model_evaluation_artifact)\n",
    "                logger.info(f\"Model accepted. Model eval artifact {model_evaluation_artifact} created\")\n",
    "\n",
    "            else:\n",
    "                logger.info(\"Trained model is no better than existing model hence not accepting trained model\")\n",
    "                model_evaluation_artifact = ModelEvaluationArtifact(evaluated_model_path=trained_model_file_path,\n",
    "                                                                    is_model_accepted=False)\n",
    "            return model_evaluation_artifact\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def __del__(self):\n",
    "        logging.info(f\"{'=' * 20}Model Evaluation log completed.{'=' * 20} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from insurence_premium.entity.model_entity import ModelTrainerArtifact\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-04-04 14:06:48,901: INFO: common]: yaml file: config\\config.yaml loaded successfully\n",
      "[2023-04-04 14:06:48,909: INFO: common]: yaml file: params.yaml loaded successfully\n",
      "[2023-04-04 14:06:48,911: INFO: common]: created directory at: artifacts\n",
      "[2023-04-04 14:06:48,928: INFO: common]: yaml file: config\\schema.yaml loaded successfully\n",
      "[2023-04-04 14:06:48,947: INFO: common]: yaml file: config\\schema.yaml loaded successfully\n",
      "[2023-04-04 14:06:48,995: INFO: common]: yaml file: config\\schema.yaml loaded successfully\n",
      "[2023-04-04 14:06:48,998: INFO: 2686975707]: Converting target column into numpy array.\n",
      "[2023-04-04 14:06:49,002: INFO: 2686975707]: Conversion completed target column into numpy array.\n",
      "[2023-04-04 14:06:49,005: INFO: 2686975707]: Dropping target column from the dataframe.\n",
      "[2023-04-04 14:06:49,010: INFO: 2686975707]: Dropping target column from the dataframe completed.\n",
      "[2023-04-04 14:06:49,013: INFO: 2686975707]: Not found any existing model. Hence accepting trained model\n",
      "[2023-04-04 14:06:49,017: INFO: 2686975707]: Previous eval result: {}\n",
      "[2023-04-04 14:06:49,018: INFO: 2686975707]: Updated eval result:{'best_model': {'model': 'artifacts/data_model_trainer/trained_model/model.pkl'}}\n",
      "[2023-04-04 14:06:49,025: INFO: 2686975707]: Model accepted. Model eval artifact ModelEvaluationArtifact(evaluated_model_path='artifacts/data_model_trainer/trained_model/model.pkl', is_model_accepted=True) created\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    con=ConfigurationManager()\n",
    "    model_evaluation=ModelEvalution(config=con ,model_trainer_artifact=ModelTrainerArtifact)\n",
    "    model_evaluation.initiate_model_evaluation()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yaml_file(file_path:str):\n",
    "    try:\n",
    "        with open(file_path ,'r') as f:\n",
    "            response =yaml.safe_load(f)\n",
    "            return response\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
