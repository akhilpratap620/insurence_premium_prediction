{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\somit\\\\Downloads\\\\project_ineuron\\\\insurence_premium_prediction'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir(\"C:\\\\Users\\somit\\Downloads\\project_ineuron\\insurence_premium_prediction\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from insurence_premium.entity import DataIngestionConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataValidationConfig:\n",
    "    root_dir: Path\n",
    "    schema_file_path:Path\n",
    "    report_file_path: Path\n",
    "    report_page_file_path: Path\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from insurence_premium.constant import *\n",
    "from insurence_premium.util.common import read_yaml ,create_directories ,save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SCHEMA_FILE_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m SCHEMA_FILE_PATH\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SCHEMA_FILE_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "SCHEMA_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self ,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config =read_yaml(config_filepath)\n",
    "        self.params =read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_data_ingestion_config(self)->DataIngestionConfig:\n",
    "        ingestion_config =self.config.data_ingestion\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=Path(ingestion_config.root_dir),\n",
    "            raw_data=Path(ingestion_config.raw_data),\n",
    "            ingested_train_dir=Path(ingestion_config.ingested_train_dir),\n",
    "            ingested_test_dir =Path(ingestion_config.ingested_test_dir)\n",
    "        )\n",
    "        return data_ingestion_config\n",
    "\n",
    "    \n",
    "\n",
    "    def get_data_validation_config(self)->DataValidationConfig:\n",
    "        validation_config =self.config.data_validation\n",
    "\n",
    "        data_validation_config =DataValidationConfig(\n",
    "            root_dir=Path(validation_config.root_dir),\n",
    "            schema_file_path=Path(validation_config.schema_file_path),\n",
    "            report_file_path=Path(validation_config.report_file_path),\n",
    "            report_page_file_path=Path(validation_config.report_page_file_path)\n",
    "            \n",
    "        )\n",
    "        return data_validation_config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-13 10:15:23,687: INFO: common]: yaml file: config\\config.yaml loaded successfully\n",
      "[2023-03-13 10:15:23,690: INFO: common]: yaml file: params.yaml loaded successfully\n",
      "[2023-03-13 10:15:23,692: INFO: common]: created directory at: artifacts\n"
     ]
    }
   ],
   "source": [
    "con= ConfigurationManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataValidationConfig(root_dir=WindowsPath('artifacts/data_validation'), schema_file_path=WindowsPath('config/schema.yaml'), report_file_path=WindowsPath('artifacts/data_validation/report.json'), report_page_file_path=WindowsPath('artifacts/data_validation/report.html'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.get_data_validation_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidation:\n",
    "    def __init__(self ,config:ConfigurationManager):\n",
    "        logger.info(\"_____________data_validation_start_____________\")\n",
    "        self.config =ConfigurationManager()\n",
    "        self.data_ingestion_config =ConfigurationManager().get_data_ingestion_config()\n",
    "        self.data_validation_config =ConfigurationManager().get_data_validation_config()\n",
    "        \n",
    "\n",
    "    def get_train_test_data(self):\n",
    "        try:\n",
    "            train_file_path=self.data_ingestion_config.ingested_train_dir\n",
    "            test_file_path=self.data_ingestion_config.ingested_test_dir\n",
    "            train_df=pd.read_csv(train_file_path ,index_col='Unnamed: 0')\n",
    "            test_df=pd.read_csv(test_file_path,index_col='Unnamed: 0')\n",
    "\n",
    "            return train_df ,test_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e    \n",
    "\n",
    "    def is_train_test_file_exist(self):\n",
    "        try:\n",
    "            is_train_file_exist=False\n",
    "            is_test_file_exist=False\n",
    "\n",
    "            train_file_path=self.data_ingestion_config.ingested_train_dir\n",
    "            test_file_path=self.data_ingestion_config.ingested_test_dir\n",
    "            logger.info(\"checking if train and test file exists\")\n",
    "            is_train_file_exist=os.path.exists(train_file_path)\n",
    "            is_test_file_exist=os.path.exists(test_file_path)\n",
    "            logger.info(\"Both training and test file path exist\")\n",
    "\n",
    "            return is_train_file_exist and is_test_file_exist\n",
    "\n",
    "            if not (is_train_file_exist and is_train_file_exist):\n",
    "                raise Exception(f\"train or test file missing\")\n",
    "            logger.info(f\"train_file: {train_file_path} ,test_file: {test_file_path}\") \n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def validate_schema(self):\n",
    "        try:\n",
    "            logger.info(\"laoding train file path\")\n",
    "            train_file_path =self.data_ingestion_config.ingested_train_dir\n",
    "            logger.info(\"started reading schema.yaml \")\n",
    "            schema_file_path=Path(self.data_validation_config.schema_file_path)\n",
    "            schema=read_yaml(schema_file_path)\n",
    "            logger.info(\"read schema.yaml successfully\")\n",
    "            logger.info(\"reading data as a data frame\")\n",
    "            df =pd.read_csv(train_file_path ,index_col='Unnamed: 0')\n",
    "            logger.info(\" started data validating according to schema data\")\n",
    "            data_type=True\n",
    "            if data_type:\n",
    "\n",
    "                for i in df.columns:\n",
    "                    df[i].dtypes==schema[i]\n",
    "                    logger.info(f\"data column: {i} is matching with schema data set\")\n",
    "            else:\n",
    "                logger.info(f\"datatypes are not maching column:{i} with schema data\")  \n",
    "            logger.info(\"data validation according to schema data is successfull\")          \n",
    "        except Exception as e:\n",
    "            raise e        \n",
    "\n",
    "    def get_and_save_data_drift_report(self):\n",
    "        try:\n",
    "            logger.info(\"initiating profile\")\n",
    "            profile =Profile(sections=[DataDriftProfileSection()])\n",
    "            logger.info(\"reading train and test data frame\")\n",
    "            train_df,test_df =self.get_train_test_data()\n",
    "            profile.calculate(train_df,test_df)\n",
    "            logger.info(\"calculating data drifting\")\n",
    "            report=json.loads(profile.json())\n",
    "            report_file_path = self.data_validation_config.report_file_path\n",
    "            report_dir = os.path.dirname(report_file_path)\n",
    "            os.makedirs(report_dir , exist_ok=True) \n",
    "            \n",
    "\n",
    "            save_json(report_file_path,report)\n",
    "            logger.info(\"report generated successfully\")\n",
    "            return report\n",
    "        except Exception as e:\n",
    "            raise e    \n",
    "\n",
    "    def save_data_drift_report_page(self):\n",
    "        try:\n",
    "            dashboard =Dashboard(tabs=[DataDriftTab()])\n",
    "            train_df,test_df =self.get_train_test_data()\n",
    "            dashboard.calculate(train_df,test_df)\n",
    "            report_page_file_path=self.data_validation_config.report_page_file_path\n",
    "            report_page_dir =os.path.dirname(report_page_file_path)\n",
    "            os.makedirs(report_page_dir , exist_ok=True)\n",
    "\n",
    "            dashboard.save(report_page_file_path)\n",
    "            logger.info(\"generated successfully\")\n",
    "        except Exception as e:\n",
    "            raise e        \n",
    "\n",
    "    def is_data_drift_found(self):\n",
    "        try:\n",
    "            report = self.get_and_save_data_drift_report()\n",
    "            self.save_data_drift_report_page()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            raise e              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C:\\\\Users\\\\somit\\\\Downloads\\\\project_ineuron\\\\insurence_premium_prediction\\\\artifacts\\data_ingestion\\\\train\\\\insurence.csv\",index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-13 10:19:56,311: INFO: common]: yaml file: config\\config.yaml loaded successfully\n",
      "[2023-03-13 10:19:56,314: INFO: common]: yaml file: params.yaml loaded successfully\n",
      "[2023-03-13 10:19:56,317: INFO: common]: created directory at: artifacts\n",
      "[2023-03-13 10:19:56,320: INFO: 3566140676]: _____________data_validation_start_____________\n",
      "[2023-03-13 10:19:56,327: INFO: common]: yaml file: config\\config.yaml loaded successfully\n",
      "[2023-03-13 10:19:56,329: INFO: common]: yaml file: params.yaml loaded successfully\n",
      "[2023-03-13 10:19:56,331: INFO: common]: created directory at: artifacts\n",
      "[2023-03-13 10:19:56,335: INFO: common]: yaml file: config\\config.yaml loaded successfully\n",
      "[2023-03-13 10:19:56,338: INFO: common]: yaml file: params.yaml loaded successfully\n",
      "[2023-03-13 10:19:56,340: INFO: common]: created directory at: artifacts\n",
      "[2023-03-13 10:19:56,344: INFO: common]: yaml file: config\\config.yaml loaded successfully\n",
      "[2023-03-13 10:19:56,349: INFO: common]: yaml file: params.yaml loaded successfully\n",
      "[2023-03-13 10:19:56,352: INFO: common]: created directory at: artifacts\n",
      "[2023-03-13 10:19:56,362: INFO: 3566140676]: checking if train and test file exists\n",
      "[2023-03-13 10:19:56,365: INFO: 3566140676]: Both training and test file path exist\n",
      "[2023-03-13 10:19:56,368: INFO: 3566140676]: laoding train file path\n",
      "[2023-03-13 10:19:56,380: INFO: 3566140676]: started reading schema.yaml \n",
      "[2023-03-13 10:19:56,383: INFO: common]: yaml file: config\\schema.yaml loaded successfully\n",
      "[2023-03-13 10:19:56,385: INFO: 3566140676]: read schema.yaml successfully\n",
      "[2023-03-13 10:19:56,387: INFO: 3566140676]: reading data as a data frame\n",
      "[2023-03-13 10:19:56,397: INFO: 3566140676]:  started data validating according to schema data\n",
      "[2023-03-13 10:19:56,399: INFO: 3566140676]: data column: age is matching with schema data set\n",
      "[2023-03-13 10:19:56,400: INFO: 3566140676]: data column: sex is matching with schema data set\n",
      "[2023-03-13 10:19:56,406: INFO: 3566140676]: data column: bmi is matching with schema data set\n",
      "[2023-03-13 10:19:56,409: INFO: 3566140676]: data column: children is matching with schema data set\n",
      "[2023-03-13 10:19:56,412: INFO: 3566140676]: data column: smoker is matching with schema data set\n",
      "[2023-03-13 10:19:56,417: INFO: 3566140676]: data column: region is matching with schema data set\n",
      "[2023-03-13 10:19:56,423: INFO: 3566140676]: data column: expenses is matching with schema data set\n",
      "[2023-03-13 10:19:56,424: INFO: 3566140676]: data validation according to schema data is successfull\n",
      "[2023-03-13 10:19:56,425: INFO: 3566140676]: initiating profile\n",
      "[2023-03-13 10:19:56,428: INFO: 3566140676]: reading train and test data frame\n",
      "[2023-03-13 10:19:56,534: INFO: 3566140676]: calculating data drifting\n",
      "[2023-03-13 10:19:56,622: INFO: common]: json file saved at: artifacts\\data_validation\\report.json\n",
      "[2023-03-13 10:19:56,629: INFO: 3566140676]: report generated successfully\n",
      "[2023-03-13 10:19:58,460: INFO: 3566140676]: generated successfully\n",
      "[2023-03-13 10:19:58,462: INFO: 3566140676]: initiating profile\n",
      "[2023-03-13 10:19:58,463: INFO: 3566140676]: reading train and test data frame\n",
      "[2023-03-13 10:19:58,517: INFO: 3566140676]: calculating data drifting\n",
      "[2023-03-13 10:19:58,521: INFO: common]: json file saved at: artifacts\\data_validation\\report.json\n",
      "[2023-03-13 10:19:58,522: INFO: 3566140676]: report generated successfully\n",
      "[2023-03-13 10:19:58,700: INFO: 3566140676]: generated successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    con_obj = ConfigurationManager()\n",
    "    data_validation_config = con_obj.get_data_validation_config()\n",
    "    data_validation = DataValidation(config=data_validation_config)\n",
    "    data_validation.get_train_test_data()\n",
    "    data_validation.is_train_test_file_exist()\n",
    "    data_validation.validate_schema()\n",
    "    data_validation.get_and_save_data_drift_report()\n",
    "    data_validation.save_data_drift_report_page()\n",
    "    data_validation.is_data_drift_found()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from insurence_premium import logger\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age           int64\n",
       "sex          object\n",
       "bmi         float64\n",
       "children      int64\n",
       "smoker       object\n",
       "region       object\n",
       "expenses    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['age', 'sex', 'bmi', 'children', 'smoker', 'region', 'expenses'], dtype='object'),\n",
       " array([dtype('int64'), dtype('O'), dtype('float64'), dtype('int64'),\n",
       "        dtype('O'), dtype('O'), dtype('float64')], dtype=object))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes.index  , df.dtypes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type =list(map(lambda x:str(x).replace(\"dtype('\" ,\"\").replace(\"')\" , \"\") ,df.dtypes.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type=list(map(lambda x:str(x).replace(\"dtype('\" ,\"\").replace(\"')\" , \"\") ,df.dtypes.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': 'int64',\n",
       " 'sex': 'object',\n",
       " 'bmi': 'float64',\n",
       " 'children': 'int64',\n",
       " 'smoker': 'object',\n",
       " 'region': 'object',\n",
       " 'expenses': 'float64'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(columns ,data_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-04 22:55:07,237: INFO: common]: yaml file: config\\schema.yaml loaded successfully\n"
     ]
    }
   ],
   "source": [
    "schema=read_yaml(Path(\"config/schema.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema['age']==df['age'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from insurence_premium import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    df[i].dtypes==schema[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_schema(df):\n",
    "    data_type=True\n",
    "    if data_type:\n",
    "\n",
    "        for i in df.columns:\n",
    "            df[i].dtypes==schema[i]\n",
    "            logger.info(f\"data column: {i} is matching with schema data set\")\n",
    "    else:\n",
    "        logger.info(\"datatypes are not maching with schema data\")        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-04 23:09:58,446: INFO: 2573481559]: data column: age is matching with schema data set\n",
      "[2023-03-04 23:09:58,448: INFO: 2573481559]: data column: sex is matching with schema data set\n",
      "[2023-03-04 23:09:58,449: INFO: 2573481559]: data column: bmi is matching with schema data set\n",
      "[2023-03-04 23:09:58,451: INFO: 2573481559]: data column: children is matching with schema data set\n",
      "[2023-03-04 23:09:58,453: INFO: 2573481559]: data column: smoker is matching with schema data set\n",
      "[2023-03-04 23:09:58,457: INFO: 2573481559]: data column: region is matching with schema data set\n",
      "[2023-03-04 23:09:58,458: INFO: 2573481559]: data column: expenses is matching with schema data set\n"
     ]
    }
   ],
   "source": [
    "validate_schema(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\somit\\Downloads\\project_ineuron\\insurence_premium_prediction\\env\\lib\\site-packages\\evidently\\analyzers\\__init__.py:3: UserWarning: analyzers are deprecated, use metrics instead\n",
      "  warnings.warn(\"analyzers are deprecated, use metrics instead\")\n",
      "c:\\Users\\somit\\Downloads\\project_ineuron\\insurence_premium_prediction\\env\\lib\\site-packages\\evidently\\model_profile\\__init__.py:8: UserWarning: model profiles are deprecated, use metrics instead\n",
      "  warnings.warn(\"model profiles are deprecated, use metrics instead\")\n",
      "c:\\Users\\somit\\Downloads\\project_ineuron\\insurence_premium_prediction\\env\\lib\\site-packages\\evidently\\dashboard\\__init__.py:8: UserWarning: dashboards are deprecated, use metrics instead\n",
      "  warnings.warn(\"dashboards are deprecated, use metrics instead\")\n"
     ]
    }
   ],
   "source": [
    "from evidently.model_profile import Profile\n",
    "from evidently.model_profile.sections import DataDriftProfileSection\n",
    "from evidently.dashboard import Dashboard\n",
    "from evidently.dashboard.tabs import DataDriftTab\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ConfigurationManager' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m con()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'ConfigurationManager' object is not callable"
     ]
    }
   ],
   "source": [
    "con."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_save_data_drift_report():\n",
    "        try:\n",
    "            profile =Profile(sections=[DataDriftProfileSection()])\n",
    "            train_df=con().get_data_ingestion().train_file_path\n",
    "            test_df= con().get_data_ingestion().train_file_path\n",
    "\n",
    "            profile.calculate(train_df,test_df)\n",
    "            report=json.loads(profile.json())\n",
    "            report_file_path = self.data_validation_config.report_file_path\n",
    "            report_dir = os.path.dirname(report_file_path)\n",
    "            os.makedirs(report_dir,exist_ok=True)    \n",
    "\n",
    "            write_json(report_file_path,report)\n",
    "            return report\n",
    "        except Exception as e:\n",
    "            raise e    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-10 10:04:39,767: INFO: common]: yaml file: config\\config.yaml loaded successfully\n",
      "[2023-03-10 10:04:39,770: INFO: common]: yaml file: params.yaml loaded successfully\n",
      "[2023-03-10 10:04:39,776: INFO: common]: created directory at: artifacts\n",
      "[2023-03-10 10:04:39,777: INFO: 1370954146]: _____________data_validation_start_____________\n",
      "[2023-03-10 10:04:39,782: INFO: common]: yaml file: config\\config.yaml loaded successfully\n",
      "[2023-03-10 10:04:39,784: INFO: common]: yaml file: params.yaml loaded successfully\n",
      "[2023-03-10 10:04:39,787: INFO: common]: created directory at: artifacts\n",
      "[2023-03-10 10:04:39,794: INFO: common]: yaml file: config\\config.yaml loaded successfully\n",
      "[2023-03-10 10:04:39,797: INFO: common]: yaml file: params.yaml loaded successfully\n",
      "[2023-03-10 10:04:39,799: INFO: common]: created directory at: artifacts\n",
      "[2023-03-10 10:04:39,805: INFO: common]: yaml file: config\\config.yaml loaded successfully\n",
      "[2023-03-10 10:04:39,810: INFO: common]: yaml file: params.yaml loaded successfully\n",
      "[2023-03-10 10:04:39,811: INFO: common]: created directory at: artifacts\n",
      "[2023-03-10 10:04:39,825: INFO: 1370954146]: checking if train and test file exists\n",
      "[2023-03-10 10:04:39,827: INFO: 1370954146]: Both training and test file path exist\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from insurence_premium.config.configuration import ConfigurationManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\somit\\Downloads\\project_ineuron\\insurence_premium_prediction\\env\\lib\\site-packages\\evidently\\analyzers\\__init__.py:3: UserWarning: analyzers are deprecated, use metrics instead\n",
      "  warnings.warn(\"analyzers are deprecated, use metrics instead\")\n",
      "c:\\Users\\somit\\Downloads\\project_ineuron\\insurence_premium_prediction\\env\\lib\\site-packages\\evidently\\model_profile\\__init__.py:8: UserWarning: model profiles are deprecated, use metrics instead\n",
      "  warnings.warn(\"model profiles are deprecated, use metrics instead\")\n",
      "c:\\Users\\somit\\Downloads\\project_ineuron\\insurence_premium_prediction\\env\\lib\\site-packages\\evidently\\dashboard\\__init__.py:8: UserWarning: dashboards are deprecated, use metrics instead\n",
      "  warnings.warn(\"dashboards are deprecated, use metrics instead\")\n"
     ]
    }
   ],
   "source": [
    "from evidently.model_profile import Profile\n",
    "from evidently.model_profile.sections import DataDriftProfileSection\n",
    "from evidently.dashboard import Dashboard\n",
    "from evidently.dashboard.tabs import DataDriftTab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
